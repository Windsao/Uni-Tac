teacher eval model

def forward_once(self, data_batch, model="student"):
    """
    Forward pass for one batch of data.
    - model: either 'student' or 'teacher', determines which encoder to use.
    """
    # Extract domains and inputs
    enc_domain = data_batch["encoder_domain"]
    dec_domain = data_batch["decoder_domain"]
    batch_x = data_batch["X"]
    
    # Denormalize function for RMSE calculation (if applicable)
    if "pose_estimation_" in dec_domain:
        label_inv_normalize = data_batch["label_inv_normalize"]
    else:
        label_inv_normalize = None
    
    # Set the forward mode (single or multi-tower based on decoder domain)
    if "electroassem" in dec_domain or "pose_estimation" in dec_domain:
        forward_mode = "multi_tower"
    else:
        forward_mode = "single_tower"
    
    self.model.set_domains(enc_domain, dec_domain, forward_mode)
    
    if model == "teacher":
        # Teacher model: Select the encoder dynamically based on encoder_domain
        teacher_encoder = self.teacher_model.encoders[enc_domain]  # Assuming encoders are stored in the teacher model
        teacher_features = teacher_encoder(batch_x.to(self.device, non_blocking=True))  # Forward through teacher encoder
        if forward_mode == "multi_tower":
            teacher_pred = self.teacher_model(*teacher_features)
        else:
            teacher_pred = self.teacher_model(teacher_features)
        return label_inv_normalize, teacher_pred

    elif model == "student":
        # Student model: Always use the fixed 'digit' encoder
        student_encoder = self.student_model.encoders['digit']  # Fixed encoder for student
        student_features = student_encoder(batch_x.to(self.device, non_blocking=True))  # Forward through student encoder
        if forward_mode == "multi_tower":
            student_pred = self.student_model(*student_features)
        else:
            student_pred = self.student_model(student_features)
        return label_inv_normalize, student_pred

    else:
        raise ValueError("Model must be either 'student' or 'teacher'")

def forward_once(self, data_batch, model="student"):
    enc_domain = data_batch["encoder_domain"]
    dec_domain = data_batch["decoder_domain"]
    batch_x = data_batch["X"]

    # 用于计算 RMSE
    if "pose_estimation_" in dec_domain:
        label_inv_normalize = data_batch["label_inv_normalize"]
    else:
        label_inv_normalize = None

    # 设置 forward mode
    if "electroassem" in dec_domain or "pose_estimation" in dec_domain:
        forward_mode = "multi_tower"
    else:
        forward_mode = "single_tower"

    if model == "teacher":
        # 动态选择教师模型的编码器
        teacher_encoder = self.teacher_mo[enc_domain] del.encoders # 选择教师的编码器
        self.teacher_model.set_domains(enc_domain, dec_domain, forward_mode)  # 设置 domain
        
        if forward_mode == "single_tower":
            Xs = batch_x.to(self.device, non_blocking=True)
            teacher_features = teacher_encoder(Xs)  # 编码器输出
            teacher_pred = self.teacher_model.decoders[self.teacher_model._decoder_domain](
                self.teacher_model.trunk(teacher_features)
            )
        else:  # multi_tower 模式
            Xs = [x.to(self.device, non_blocking=True) for x in batch_x]
            teacher_features = [teacher_encoder(x) for x in Xs]  # 逐个编码
            teacher_pred = self.teacher_model.decoders[self.teacher_model._decoder_domain](
                *[self.teacher_model.trunk(f) for f in teacher_features]
            )

        return label_inv_normalize, teacher_pred, teacher_features  # 现在返回 encoder 的输出

    elif model == "student":
        # 学生模型始终使用 'digit' 作为编码器
        student_encoder = self.student_model.encoders["digit"]
        self.student_model.set_domains("digit", dec_domain, forward_mode)  # 这里 encoder domain 固定为 'digit'
        
        if forward_mode == "single_tower":
            Xs = batch_x.to(self.device, non_blocking=True)
            student_features = student_encoder(Xs)  # 编码器输出
            student_pred = self.student_model.decoders[self.student_model._decoder_domain](
                self.student_model.trunk(student_features)
            )
        else:
            Xs = [x.to(self.device, non_blocking=True) for x in batch_x]
            student_features = [student_encoder(x) for x in Xs]
            student_pred = self.student_model.decoders[self.student_model._decoder_domain](
                *[self.student_model.trunk(f) for f in student_features]
            )

        return label_inv_normalize, student_pred, student_features  # 现在返回 encoder 的输出

    else:
        raise ValueError("Model must be either 'student' or 'teacher'")


@torch.no_grad()
def test(self, test_steps, run_id, cur_step, enable_wandb):
    # 使用学生模型进行评估
    self.student_model.to(self.device)
    test_iter = iter(self.eval_dataloader)
    self.student_model.eval()
    losses = []
    test_loss_history = {"all_losses": []}

    pbar = tqdm(range(test_steps), position=0, leave=True)
    for idx in pbar:
        data = next(test_iter)
        enc_domain = data["encoder_domain"]
        dec_domain = data["decoder_domain"]
        batch_y = data["Y"]
        # 获取反归一化函数（如果需要）
        inv_normalize_func = data["inv_normalize"]
        
        # 使用学生模型的 forward_once 方法进行前向传播
        label_inv_normalize, pred = self.forward_once(data, model="student")
        Y = batch_y.to(self.device)

        # 计算损失 (使用学生模型的损失函数)
        loss = self.student_model.compute_loss(pred, Y)
        losses.append(loss.item())

        self.compose_loss_history(test_loss_history, enc_domain, dec_domain, loss, pred, batch_y, denormalize_func=label_inv_normalize)

        # 可选：生成 MAE 可视化
        if get_entry_or(self.cfg.train, "generate_mae_visualizations", True) and idx == 0 and "mae" in dec_domain:
            (pred_imgs, mask, ids_restore) = pred
            self.generate_mae_visualizations(
                data["X"].to(self.device, non_blocking=True), 
                self.cfg.network.patch_size, pred_imgs, mask, inv_normalize_func, run_id, cur_step
            )
        
        pbar.set_description(f"Test {idx}/{test_steps} steps | loss: {loss.item():.4f}")
    if enable_wandb:
        log_items = {
            "eval/epoch": cur_step // len(self.train_dataloader),
            "eval/step": cur_step,
            f"eval/avg_test_loss": np.mean(losses)
        }
        for k, v in test_loss_history.items():
            log_items[f"eval/{k}"] = np.mean(v)
        wandb.log(log_items)

    return test_loss_history
