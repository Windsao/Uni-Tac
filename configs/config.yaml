comment: "" # a comment that will be appended to the wandb run name

train:
  # img_size: 224 # TODO: not being used
  batch_size: 64
  dl_weight_type: "root" # how each dataloader is weighted according to number of batches. "equal", "invlinear", "root"
  num_data_workers: 0
  wandb: true
  wandb_entity: "magicslabnorthwestern" # your wandb username
  log_freq: 10 # how often to log to wandb
  save_model: true
  finetune_from: "" # path to a model to finetune / load from
  teacher_ckpt: "/nyx-storage1/hanliu/Uni-Tac/tac_ckpt/t3_medium" # path to teacher model

  # Will train for total_train_steps, during which will run eval for test_steps every test_every steps
  total_train_steps: 10000 #00000
  test_every: 750 #750
  test_steps: 10 #10
  generate_mae_visualizations: true

  # whether to freeze the encoder and trunk
  freeze_encoder: false
  freeze_trunk: true
  # whether to unfreeze the encoder and trunk at a given step. only effective when both freeze_encoder and freeze_trunk are true
  scheduled_unfreeze: false 
  scheduled_unfreeze_step: 20000

  optimizer:
    _target_: torch.optim.AdamW
    lr: 1.0e-4
    eps: 1.0e-6
    weight_decay: 0.1
  # the head and stem are updated at different frequencies. they can be trained with less learning rates.
  nontrunk_lr_scale: 1.0 # 0.5

  scheduler:
    _target_: torch.optim.lr_scheduler.CosineAnnealingLR
    T_max: ${train.total_train_steps}
    eta_min: 1e-8

defaults:
  - _self_
  - /network: pretrain1_uni_mae
  - /teacher_network: pretrain1_mae
  - /datasets: 
    - single_tower_mae
  #   # - cnc_mae
  #   # - panda_mae_autogen
  #   # - new_mae_data
  #   # - new_mae_data_big
  #   # - new_mae_Invariant
