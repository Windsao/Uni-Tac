comment: "" # a comment that will be appended to the wandb run name

train:
  # img_size: 224 # TODO: not being used
  batch_size: 64
  dl_weight_type: "root" # how each dataloader is weighted according to number of batches. "equal", "invlinear", "root"
  num_data_workers: 0
  wandb: false
  wandb_entity: "saffina" # your wandb username
  log_freq: 10 # how often to log to wandb
  save_model: false
  finetune_from: "checkpoints/best_2025-01-08_17_53_03-" # path to a model to finetune / load from
  # best_2025-01-06_14_11_15- best_2025-01-08_17_53_03-
  # Will train for total_train_steps, during which will run eval for test_steps every test_every steps
  total_train_steps: 1 #00000
  test_every: 1 #750
  test_steps: 1 #10
  generate_mae_visualizations: true

  # whether to freeze the encoder and trunk
  freeze_encoder: true
  freeze_trunk: true
  # whether to unfreeze the encoder and trunk at a given step. only effective when both freeze_encoder and freeze_trunk are true
  scheduled_unfreeze: false 
  scheduled_unfreeze_step: 20000

  optimizer:
    _target_: torch.optim.AdamW
    lr: 1.0e-4
    eps: 1.0e-6
    weight_decay: 0.1
  # the head and stem are updated at different frequencies. they can be trained with less learning rates.
  nontrunk_lr_scale: 1.0 # 0.5

  scheduler:
    _target_: torch.optim.lr_scheduler.CosineAnnealingLR
    T_max: ${train.total_train_steps}
    eta_min: 1e-8

defaults:
  - _self_
  - /network: pretrain1_mae
  - /datasets: 
    - single_tower_mae
    # - cnc_mae
    # - panda_mae_autogen
    # - new_mae_data
    # - new_mae_data_big
    # - new_mae_Invariant
